# Lilly-X: Reasoning GraphRAG Architect ğŸ§ ğŸ•¸ï¸


Lilly-X is a local, privacy-first RAG system that **thinks before it searches**. It combines **Query Decomposition** (Reasoning) with **Hybrid Retrieval** (Vector + Graph) to answer complex engineering questions.

## ğŸš€ Capabilities (Level 3 GraphRAG)

**Microsoft-style GraphRAG with Community Detection** - Lilly-X implements advanced multi-tier retrieval:

- **ğŸŒ Global Search:** Can answer abstract questions like *"What are the main themes?"* by synthesizing Community Summaries instead of individual chunks. Uses keyword-based community retrieval.
  
- **ğŸ”¬ Community Detection:** Automatically clusters entities using the **Leiden Algorithm** (via Neo4j GDS). Example: Detected **8 communities** in initial tests, each with LLM-generated summaries and keywords.

- **ğŸ”€ Hybrid Retrieval:** Intelligently combines three search strategies:
  - **Vector Search** (Qdrant) for semantic similarity
  - **Graph Traversal** (Neo4j) for entity relationships
  - **Community Context** for high-level thematic queries

- **ğŸ¯ Intent-Based Routing:** Automatically detects query intent (GLOBAL_DISCOVERY vs specific questions) and routes to the appropriate retrieval strategy.

### âš ï¸ Prerequisites

- **Neo4j 5.x with GDS Plugin:** Community detection requires the [Graph Data Science library](https://neo4j.com/docs/graph-data-science/current/)
- **Python 3.10 - 3.12:** Python 3.14 is NOT supported yet (Pydantic v1 compatibility issues)
- See `.python-version` file for recommended version


## ğŸ—ï¸ Architecture

```mermaid
graph TD
    User[ğŸ‘¤ User Input] --> UI[ğŸ–¥ï¸ Streamlit UI]
    UI --> Engine[ğŸ§  RAG Engine]
    
    subgraph Brain["ğŸ§  The Brain - Reasoning Layer"]
        Engine --> Planner[Query Planner]
        Planner -->|Decompose| SubQ[Sub-Queries]
    end
    
    subgraph Eyes["ğŸ‘ï¸ The Eyes - Retrieval Layer"]
        SubQ --> Vector[Qdrant<br/>Dense Vector Search]
        SubQ --> Graph[Neo4j<br/>Knowledge Graph]
    end
    
    subgraph Synthesis["ğŸ”¬ Synthesis Layer"]
        Vector --> Context[Aggregated Context]
        Graph --> Context
        Context --> LLM[Ollama<br/>Local LLM]
        LLM --> Answer[âœ¨ Final Answer]
    end
    
    Answer --> UI
    
    style Brain fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    style Eyes fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    style Synthesis fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px
```

## ğŸ”„ The Reasoning Loop

```mermaid
sequenceDiagram
    participant U as ğŸ‘¤ User
    participant A as Lilly-X UI
    participant P as Query Planner (LLM)
    participant D as Data Stores

    U->>A: "Compare LoRA and QLoRA"
    A->>P: Analyze Intent & Plan
    P-->>A: Plan: [Define LoRA, Define QLoRA, Compare]
    
    loop For each Sub-Query
        A->>D: Hybrid Search (Vector + Graph)
        D-->>A: Retrieved Facts
    end
    
    A->>A: Synthesize & Cite Sources
    A-->>U: ğŸ“ Structured Response
```

## ğŸš€ Key Features

- **ğŸ§  Query Decomposition**: Complex questions broken into atomic sub-queries
- **ğŸ•¸ï¸ Hybrid Retrieval**: Combines dense vector search (Qdrant) with symbolic knowledge graphs (Neo4j)
- **ğŸ¯ Intent Classification**: Automatically identifies factual, workflow, or comparison queries
- **ğŸ“Š Thinking Process UI**: Visual sidebar showing reasoning steps in real-time
- **ğŸ”’ 100% Local**: All LLM inference runs on your hardware via Ollama
- **ğŸ”§ Robust JSON Parsing**: Uses `json_repair` for fault-tolerant LLM output handling

## ğŸ› ï¸ Quick Start

### Prerequisites

- **Python 3.10 - 3.12** (âš ï¸ Python 3.14 is NOT supported - see [Capabilities](#-capabilities-level-3-graphrag))
- **Neo4j 5.x with GDS Plugin** for community detection (see [Setup Guide](NEO4J_GDS_SETUP.md))
- **Podman** or Docker for databases
- **Ollama** installed with models: `mistral-nemo:12b`, `nomic-embed-text`


### Installation

```bash
# 1. Clone and navigate to project
git clone <your-repo-url>
cd LLIX

# 2. Start Infrastructure (Neo4j + Qdrant)
podman-compose up -d

# Check containers are running
podman ps

# 3. Setup Python Environment
python3.11 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt

# 4. Configure Environment
cp .env.template .env
# Edit .env if you need to change defaults

# 5. Ingest Your Documents
# Place PDFs in ./data/docs/
python -m src.ingest

# 6. Launch Lilly-X
streamlit run src/app.py
```

Visit `http://localhost:8501` to start querying! ğŸ‰

## ğŸ“ Project Structure

```
LLIX/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py              # Streamlit UI with reasoning visualization
â”‚   â”œâ”€â”€ rag_engine.py       # Core hybrid retrieval engine
â”‚   â”œâ”€â”€ ingest.py           # Document ingestion pipeline
â”‚   â”œâ”€â”€ prompts.py          # LLM prompt templates
â”‚   â”œâ”€â”€ schemas.py          # QueryPlan and data models
â”‚   â”œâ”€â”€ graph_ops.py        # Neo4j entity resolution
â”‚   â”œâ”€â”€ memory.py           # Conversation history manager
â”‚   â””â”€â”€ config.py           # Centralized configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ docs/               # Place your PDFs here
â”œâ”€â”€ compose.yaml            # Podman/Docker infrastructure
â”œâ”€â”€ .env                    # Environment configuration
â””â”€â”€ requirements.txt        # Python dependencies
```

## âš™ï¸ Configuration

Key settings in `.env`:

```ini
# LLM Settings
LLM_MODEL=mistral-nemo:12b
EMBEDDING_MODEL=BAAI/bge-m3

# Retrieval Strategy (semantic | sentence_window | hierarchical)
RETRIEVAL_STRATEGY=semantic

# Database Connections
NEO4J_URL=bolt://localhost:7687
NEO4J_PASSWORD=password
QDRANT_URL=http://localhost:6333

# Performance
TOP_K=3
TOP_K_FINAL=5
BATCH_SIZE=16
```

## ğŸ¨ UI Features

- **ğŸ§  Agent Reasoning Sidebar**: Live display of query decomposition
  - Shows sub-queries with intent badges (ğŸ“Š Factual, âš™ï¸ Workflow, âš–ï¸ Comparison)
  - "Direct Retrieval" indicator for simple queries
- **âš¡ Live Token Streaming**: Real-time generation with tokens/second metrics
- **ğŸ“Š Performance Metrics**: Detailed timing breakdown (retrieval vs generation)
- **ğŸ“„ Source Attribution**: Expandable source nodes with scores and metadata
- **ğŸ” Debug Context**: Full transparency into retrieval decisions

## ğŸ› Troubleshooting

### Neo4j Permission Denied (Podman/SELinux)

If you see `chown: cannot read directory '/data': Permission denied`:

```bash
# Stop containers
podman-compose down

# The compose.yaml already has :Z suffix for SELinux
# Just recreate the containers
podman-compose up -d
```

### Query Planning Fails

If you see JSON parsing errors, ensure:
- Ollama is running: `ollama list`
- Model is loaded: `ollama pull mistral-nemo:12b`
- Check logs: The system has fallback to simple query if decomposition fails

### Python 3.13+ Build Errors

```bash
# Install Python 3.11
sudo apt install python3.11 python3.11-venv  # Ubuntu/Debian
# or
brew install python@3.11  # macOS

# Recreate virtual environment
rm -rf venv
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## ğŸ”’ Privacy & Security

- âœ… **100% Local Execution**: All LLM inference runs on your hardware via Ollama
- âœ… **No External Services**: Documents never leave your machine
- âœ… **Full Data Control**: No API keys or cloud dependencies required
- âœ… **Open Source Stack**: Qdrant, Neo4j, Ollama, LlamaIndex

## ï¿½ Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **LLM** | Ollama (Mistral-Nemo 12B) | Query planning & answer generation |
| **Embeddings** | BGE-M3 (HuggingFace) | Dense vector representations |
| **Vector DB** | Qdrant | Semantic similarity search |
| **Graph DB** | Neo4j | Entity relationships & knowledge graph |
| **Framework** | LlamaIndex | RAG orchestration |
| **UI** | Streamlit | Interactive chat interface |
| **Orchestration** | Podman/Docker Compose | Infrastructure management |

## â„¹ï¸ Project Status

This project is a **Proof of Concept (PoC)** designed for educational purposes and architectural demonstration. It serves as a reference implementation for **Advanced RAG patterns** including:

- Query Decomposition (Reasoning-before-Retrieval)
- Hybrid Vector + Graph Retrieval
- Multi-turn conversation with entity disambiguation
- Sentence Window & Hierarchical chunking strategies

Feedback and discussions are welcome via [Issues](https://github.com/UrbanElephant/lilly-x/issues).

---

**Built with â¤ï¸ for privacy-conscious AI enthusiasts**
