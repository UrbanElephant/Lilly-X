#!/usr/bin/env /usr/bin/python3.12
"""
Advanced RAG Pipeline - Integration Test with Mocked Components

Tests the complete pipeline flow (Decomposition ‚Üí Retrieval ‚Üí Fusion ‚Üí Rerank)
without requiring live Qdrant/Neo4j instances.

Uses MockRetriever to simulate database responses and verify logic flow.
"""

import asyncio
import sys
from pathlib import Path
from typing import List

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from llama_index.core import Settings
from llama_index.core.base.base_retriever import BaseRetriever
from llama_index.core.schema import NodeWithScore, QueryBundle, TextNode


# ============================================================================
# MOCK COMPONENTS (for testing without live databases)
# ============================================================================

class MockRetriever(BaseRetriever):
    """Mock retriever that returns dummy nodes for testing.
    
    Simulates hybrid retrieval by returning different nodes based on query.
    """
    
    def __init__(self, num_results: int = 10):
        """Initialize mock retriever.
        
        Args:
            num_results: Number of results to return per query
        """
        self.num_results = num_results
        self._call_count = 0
        super().__init__()
    
    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        """Return mock nodes with varying content and scores.
        
        Args:
            query_bundle: Query information
            
        Returns:
            List of mock NodeWithScore objects
        """
        self._call_count += 1
        query = query_bundle.query_str
        
        # Generate mock nodes
        nodes = []
        for i in range(self.num_results):
            # Vary content based on query to simulate different retrievals
            query_hash = hash(query) % 100
            
            text = f"Mock document {i+1} for query: '{query[:30]}...' "
            text += f"This document discusses topics related to the query. "
            text += f"Content variation: {query_hash + i}. "
            text += f"Generated by MockRetriever (call #{self._call_count})."
            
            node = TextNode(
                text=text,
                id_=f"mock_node_{query_hash}_{i}",
                metadata={
                    "source": "MockRetriever",
                    "query": query[:50],
                    "position": i,
                    "call_count": self._call_count,
                }
            )
            
            # Decreasing scores
            score = 1.0 - (i * 0.05)
            
            nodes.append(NodeWithScore(node=node, score=score))
        
        return nodes


class MockLLM:
    """Mock LLM for query transformation.
    
    Returns predictable responses for testing.
    """
    
    def complete(self, prompt: str) -> "MockResponse":
        """Generate mock completion.
        
        Args:
            prompt: Input prompt
            
        Returns:
            Mock response object
        """
        # Detect type of request
        if "sub-questions" in prompt.lower() or "decompose" in prompt.lower():
            # Query decomposition
            response_text = '["What is the main topic?", "What are the key details?", "What is the context?"]'
        
        elif "hypothetical" in prompt.lower() or "hyde" in prompt.lower():
            # HyDE generation
            response_text = "This is a hypothetical answer to the query. It provides detailed information about the topic."
        
        elif "alternative" in prompt.lower() or "rewrite" in prompt.lower():
            # Query rewriting
            response_text = '["similar question", "rephrased query"]'
        
        else:
            # Generic response
            response_text = "Generic LLM response for testing."
        
        return MockResponse(response_text)


class MockResponse:
    """Mock LLM response object."""
    
    def __init__(self, text: str):
        self.text = text


# ============================================================================
# TEST FUNCTIONS
# ============================================================================

async def test_pipeline_basic():
    """Test basic pipeline execution with mock components."""
    
    print("=" * 80)
    print("TEST 1: Basic Pipeline Execution")
    print("=" * 80)
    
    # Setup mock LLM
    Settings.llm = MockLLM()
    
    # Create mock retriever
    mock_retriever = MockRetriever(num_results=10)
    
    # Import pipeline (after setting up mocks)
    from advanced_rag.pipeline import AdvancedRAGPipeline
    
    # Initialize pipeline with mock retriever
    # Note: We can't use HybridRetriever without a VectorStoreIndex,
    # so we'll monkey-patch the retriever
    pipeline = AdvancedRAGPipeline(
        vector_index=None,  # No index needed
        enable_decomposition=True,
        enable_hyde=False,
        enable_rewriting=False,
        verbose=True,
    )
    
    # Monkey-patch the hybrid retriever with our mock
    pipeline.hybrid_retriever = mock_retriever
    
    # Run pipeline
    test_query = "What is Retrieval Augmented Generation?"
    
    try:
        results = await pipeline.run(
            query=test_query,
            top_k=25,
            top_n=5,
        )
        
        print("\n" + "‚îÄ" * 80)
        print("TEST RESULTS")
        print("‚îÄ" * 80)
        print(f"‚úÖ Pipeline executed successfully")
        print(f"‚úÖ Returned {len(results)} results")
        print(f"‚úÖ Mock retriever called {mock_retriever._call_count} times")
        
        print(f"\nTop Results:")
        for i, node_with_score in enumerate(results[:3], 1):
            print(f"\n{i}. Score: {node_with_score.score:.4f}")
            print(f"   Text: {node_with_score.node.text[:100]}...")
            print(f"   Source: {node_with_score.node.metadata.get('source', 'Unknown')}")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Pipeline execution failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_pipeline_error_handling():
    """Test pipeline error handling and fallbacks."""
    
    print("\n" + "=" * 80)
    print("TEST 2: Error Handling & Fallbacks")
    print("=" * 80)
    
    from advanced_rag.pipeline import AdvancedRAGPipeline
    
    # Test 1: Pipeline without retriever should raise error
    print("\nüìù Test 2.1: No retriever configured")
    try:
        pipeline = AdvancedRAGPipeline(
            vector_index=None,
            verbose=False,
        )
        
        # This should raise ValueError
        results = await pipeline.run("test query")
        print("   ‚ùå Should have raised ValueError")
        return False
        
    except ValueError as e:
        print(f"   ‚úÖ Correctly raised ValueError: {e}")
    
    # Test 2: Pipeline with retriever that returns empty results
    print("\nüìù Test 2.2: Empty retrieval results")
    
    class EmptyRetriever(BaseRetriever):
        def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
            return []
    
    pipeline = AdvancedRAGPipeline(vector_index=None, verbose=False)
    pipeline.hybrid_retriever = EmptyRetriever()
    Settings.llm = MockLLM()
    
    try:
        results = await pipeline.run("test query", top_n=5)
        
        if len(results) == 0:
            print("   ‚úÖ Correctly handled empty results")
        else:
            print(f"   ‚ö†Ô∏è  Expected 0 results, got {len(results)}")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Unexpected error: {e}")
    
    print("\n‚úÖ Error handling tests completed")
    return True


async def test_pipeline_feature_flags():
    """Test pipeline with different feature combinations."""
    
    print("\n" + "=" * 80)
    print("TEST 3: Feature Flags")
    print("=" * 80)
    
    from advanced_rag.pipeline import AdvancedRAGPipeline
    
    Settings.llm = MockLLM()
    mock_retriever = MockRetriever(num_results=8)
    
    feature_combinations = [
        {"decomposition": True, "hyde": False, "rewriting": False},
        {"decomposition": False, "hyde": True, "rewriting": False},
        {"decomposition": True, "hyde": True, "rewriting": True},
        {"decomposition": False, "hyde": False, "rewriting": False},
    ]
    
    for i, features in enumerate(feature_combinations, 1):
        print(f"\nüìù Test 3.{i}: {features}")
        
        pipeline = AdvancedRAGPipeline(
            vector_index=None,
            enable_decomposition=features["decomposition"],
            enable_hyde=features["hyde"],
            enable_rewriting=features["rewriting"],
            verbose=False,
        )
        pipeline.hybrid_retriever = MockRetriever(num_results=5)
        
        try:
            results = await pipeline.run("test query", top_n=3)
            print(f"   ‚úÖ Executed: {len(results)} results")
        except Exception as e:
            print(f"   ‚ùå Failed: {e}")
            return False
    
    print("\n‚úÖ All feature flag combinations work")
    return True


async def test_sync_wrapper():
    """Test synchronous wrapper."""
    
    print("\n" + "=" * 80)
    print("TEST 4: Synchronous Wrapper")
    print("=" * 80)
    
    from advanced_rag.pipeline import AdvancedRAGPipeline
    
    Settings.llm = MockLLM()
    
    pipeline = AdvancedRAGPipeline(
        vector_index=None,
        enable_decomposition=False,
        verbose=False,
    )
    pipeline.hybrid_retriever = MockRetriever(num_results=5)
    
    try:
        # Use sync wrapper
        results = pipeline.run_sync("test query", top_n=3)
        print(f"‚úÖ Sync wrapper executed: {len(results)} results")
        return True
    except Exception as e:
        print(f"‚ùå Sync wrapper failed: {e}")
        return False


# ============================================================================
# MAIN TEST RUNNER
# ============================================================================

async def run_all_tests():
    """Run all integration tests."""
    
    print("\n" + "#" * 80)
    print("# Advanced RAG Pipeline - Integration Tests")
    print("# Using Mock Components")
    print("#" * 80)
    
    print(f"\nPython: {sys.version}")
    print(f"Python Path: {sys.executable}")
    
    # Run tests
    tests = [
        ("Basic Pipeline Execution", test_pipeline_basic()),
        ("Error Handling", test_pipeline_error_handling()),
        ("Feature Flags", test_pipeline_feature_flags()),
        ("Sync Wrapper", test_sync_wrapper()),
    ]
    
    results = []
    for test_name, test_coro in tests:
        try:
            result = await test_coro
            results.append((test_name, result))
        except Exception as e:
            print(f"\n‚ùå Test '{test_name}' crashed: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # Summary
    print("\n" + "=" * 80)
    print("TEST SUMMARY")
    print("=" * 80)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASSED" if result else "‚ùå FAILED"
        print(f"{test_name:<40} {status}")
    
    print("\n" + "=" * 80)
    print(f"Results: {passed}/{total} tests passed")
    print("=" * 80)
    
    if passed == total:
        print("\nüéâ ALL TESTS PASSED")
        print("‚úÖ Pipeline logic flow validated")
        print("‚úÖ Error handling confirmed")
        print("‚úÖ Feature flags working")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} TESTS FAILED")
        return 1


def main():
    """Main entry point."""
    exit_code = asyncio.run(run_all_tests())
    sys.exit(exit_code)


if __name__ == "__main__":
    main()
